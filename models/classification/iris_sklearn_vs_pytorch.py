# Autogenerated from Claude

import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import torch.nn.functional as F

# Load and prepare the dataset
print("Loading Iris dataset...")
iris = load_iris()
X, y = iris.data, iris.target
X, y = iris.data, iris.target

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"Dataset shape: {X.shape}")
print(f"Number of classes: {len(np.unique(y))}")
print(f"Training set size: {X_train.shape[0]}")
print(f"Test set size: {X_test.shape[0]}")
print("-" * 60)

# =============================================================================
# SCIKIT-LEARN APPROACH
# =============================================================================
print("SCIKIT-LEARN NEURAL NETWORK")
print("=" * 40)

# Create and train the model
sklearn_model = MLPClassifier(
    hidden_layer_sizes=(64, 32),  # Two hidden layers with 64 and 32 neurons
    activation='relu',
    solver='adam',
    max_iter=1000,
    random_state=42,
    learning_rate_init=0.001
)

# Training is done in one line
print("Training sklearn model...")
sklearn_model.fit(X=X_train_scaled, y=y_train)

# Make predictions
sklearn_predictions = sklearn_model.predict(X=X_test_scaled)
sklearn_probabilities = sklearn_model.predict_proba(X=X_test_scaled)

# Calculate metrics
sklearn_accuracy = accuracy_score(y_true=y_test, y_pred=sklearn_predictions)

print(f"Sklearn Accuracy: {sklearn_accuracy:.4f}")
print("\nSklearn Classification Report:")
print(classification_report(y_true=y_test, y_pred=sklearn_predictions, target_names=iris.target_names))
print("\nSklearn Confusion Matrix:")
print(confusion_matrix(y_true=y_test, y_pred=sklearn_predictions))

print("-" * 60)

# =============================================================================
# PYTORCH APPROACH
# =============================================================================
print("PYTORCH NEURAL NETWORK")
print("=" * 40)

# Define the neural network architecture
class IrisClassifier(nn.Module):
    def __init__(self, input_size: int, hidden1_size: int, hidden2_size: int, num_classes: int):
        """
        Initialize the neural network layers.
        Args:
            input_size (int): Number of input features.
            hidden1_size (int): Number of neurons in the first hidden layer.
            hidden2_size (int): Number of neurons in the second hidden layer.
            num_classes (int): Number of output classes.
        """
        super(IrisClassifier, self).__init__()
        # First fully connected layer
        self.fc1 = nn.Linear(input_size, hidden1_size)
        # Second fully connected layer
        self.fc2 = nn.Linear(hidden1_size, hidden2_size)
        # Output layer
        self.fc3 = nn.Linear(hidden2_size, num_classes)
        # Dropout for regularization
        self.dropout = nn.Dropout(0.2)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass through the network.
        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, input_size).
        Returns:
            torch.Tensor: Output logits for each class.
        """
        # Pass through first layer and apply ReLU activation
        x = F.relu(self.fc1(x))
        # Apply dropout
        x = self.dropout(x)
        # Pass through second layer and apply ReLU activation
        x = F.relu(self.fc2(x))
        # Apply dropout
        x = self.dropout(x)
        # Output layer (no activation, as CrossEntropyLoss expects logits)
        x = self.fc3(x)
        return x

 # Convert numpy arrays to PyTorch tensors for model input
X_train_tensor = torch.FloatTensor(X_train_scaled)  # Training features
X_test_tensor = torch.FloatTensor(X_test_scaled)    # Test features
y_train_tensor = torch.LongTensor(y_train)          # Training labels
y_test_tensor = torch.LongTensor(y_test)            # Test labels

# Create a TensorDataset and DataLoader for batching
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)

# Initialize the PyTorch model with explicit arguments
pytorch_model = IrisClassifier(
    input_size=4,      # Number of input features in Iris dataset
    hidden1_size=64,   # Number of neurons in first hidden layer
    hidden2_size=32,   # Number of neurons in second hidden layer
    num_classes=3      # Number of output classes
)

# Define the loss function (cross-entropy for classification)
criterion = nn.CrossEntropyLoss()
# Define the optimizer (Adam)
optimizer = optim.Adam(params=pytorch_model.parameters(), lr=0.001)

# Training loop for the PyTorch model
print("Training PyTorch model...")
pytorch_model.train()  # Set model to training mode
num_epochs = 1000      # Number of training epochs

for epoch in range(num_epochs):
    total_loss = 0
    for batch_X, batch_y in train_loader:
        # Forward pass: compute model output
        outputs = pytorch_model(input=batch_X)
        # Compute loss between output and true labels
        loss = criterion(input=outputs, target=batch_y)

        # Backward pass: compute gradients
        optimizer.zero_grad()
        loss.backward()
        # Update model weights
        optimizer.step()

        total_loss += loss.item()

    # Print average loss every 200 epochs
    if (epoch + 1) % 200 == 0:
        avg_loss = total_loss / len(train_loader)
        print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')

# Evaluation
# Switch model to evaluation mode (disables dropout)
pytorch_model.eval()
with torch.no_grad():
    # Forward pass on test data to get raw output logits
    pytorch_outputs = pytorch_model(input=X_test_tensor)
    # Convert logits to probabilities using softmax
    pytorch_probabilities = F.softmax(input=pytorch_outputs, dim=1)
    # Get predicted class by selecting the max logit for each sample
    _, pytorch_predictions = torch.max(input=pytorch_outputs, dim=1)

    # Convert predictions and probabilities to numpy arrays for metric calculation
    pytorch_predictions_np = pytorch_predictions.numpy()
    pytorch_probabilities_np = pytorch_probabilities.numpy()

# Calculate metrics
pytorch_accuracy = accuracy_score(y_true=y_test, y_pred=pytorch_predictions_np)

print(f"\nPyTorch Accuracy: {pytorch_accuracy:.4f}")
print("\nPyTorch Classification Report:")
print(classification_report(y_true=y_test, y_pred=pytorch_predictions_np, target_names=iris.target_names))
print("\nPyTorch Confusion Matrix:")
print(confusion_matrix(y_true=y_test, y_pred=pytorch_predictions_np))

# =============================================================================
# COMPARISON SUMMARY
# =============================================================================
print("\n" + "=" * 60)
print("COMPARISON SUMMARY")
print("=" * 60)

print("Key Syntax Differences:")
print("\n1. Model Definition:")
print("   - Sklearn: MLPClassifier(hidden_layer_sizes=(64, 32))")
print("   - PyTorch: Custom class inheriting from nn.Module")

print("\n2. Training:")
print("   - Sklearn: model.fit(X, y) - one line!")
print("   - PyTorch: Manual training loop with batches, loss calculation, backprop")

print("\n3. Prediction:")
print("   - Sklearn: model.predict(X)")
print("   - PyTorch: model(X) with torch.max() for class prediction")

print("\n4. Data Handling:")
print("   - Sklearn: Works directly with numpy arrays")
print("   - PyTorch: Requires tensor conversion and data loaders")

print(f"\nPerformance Comparison:")
print(f"Sklearn Accuracy:  {sklearn_accuracy:.4f}")
print(f"PyTorch Accuracy:  {pytorch_accuracy:.4f}")

print("\nWhen to use which:")
print("- Sklearn: Quick prototyping, standard ML tasks, less code")
print("- PyTorch: Custom architectures, research, more control over training")

# Show some sample predictions
print("\nSample Predictions on Test Set:")
print("True Label | Sklearn Pred | PyTorch Pred | Class Name")
print("-" * 55)
for i in range(min(10, len(y_test))):
    true_class = iris.target_names[y_test[i]]
    sklearn_pred = iris.target_names[sklearn_predictions[i]]
    pytorch_pred = iris.target_names[pytorch_predictions_np[i]]
    print(f"{y_test[i]:10d} | {sklearn_predictions[i]:12d} | {pytorch_predictions_np[i]:12d} | {true_class}")